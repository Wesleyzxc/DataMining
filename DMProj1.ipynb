{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all packages and define functions used for data-preprocessing \n",
    "# Reads the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import all packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import tree, neighbors, svm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "import warnings\n",
    "# To ignore any future warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Repeated functions\n",
    "def getMode(df, attribute, condAtt, cond):\n",
    "    return (df[attribute][df[condAtt]==cond].mode()[0])\n",
    "\n",
    "def replaceWithMode(df, attribute, condAtt, cond):\n",
    "    mode = getMode(df, attribute, condAtt, cond)\n",
    "    df[attribute] = df[attribute].mask(((df[condAtt]==cond) & (df[attribute]=='?')), mode)\n",
    "    \n",
    "\n",
    "# read the dataset and set skipinitialspace to true to be able to .replace\n",
    "df = pd.read_csv('./HouseholderAtRisk(1).csv', skipinitialspace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 \n",
    "**1)      What proportion of households who have high risk?**\n",
    "\n",
    "Properties with high risks are calculated with value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of high risk = 0.7624690617265432\n"
     ]
    }
   ],
   "source": [
    "# Task 1 question 1\n",
    "# Show proportion of high risk\n",
    "risks = df['AtRisk'].value_counts()\n",
    "print(\"Proportion of high risk = \" + str(risks[0]/len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "**2) Did you have to fix any data quality problems? Detail them?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with multiple columns containing NaN values\n",
    "df = df.dropna(subset=[\"Relationship\", \"Sex\", \"NumYearsEducation\"], how='all')\n",
    "\n",
    "# Replace inconsistency in CountryOfOrigin\n",
    "df['CountryOfOrigin'] = df['CountryOfOrigin'].replace(\"US\", \"USA\").replace(\"United-States\", \"USA\")\n",
    "\n",
    "##Replace ? to mode in CountryOfOrigin\n",
    "modeCountry = df['CountryOfOrigin'].mode()[0]\n",
    "df.loc[df['CountryOfOrigin']=='?', 'CountryOfOrigin'] = modeCountry\n",
    "\n",
    "# Removing Gender to use numerical binary for Sex where 0 = Male, 1 = Female\n",
    "df.drop('Gender', axis=1, inplace=True)\n",
    "\n",
    "# Replace -1 value in Age with mean value\n",
    "modeAge = df['Age'].mode()[0]\n",
    "df.loc[df['Age']==-1, 'Age'] = modeAge\n",
    "\n",
    "# Round off age\n",
    "df['Age'] = df['Age'].astype(int)\n",
    "\n",
    "# Drop race as there are 39954 NaN vs 45 labelled classes\n",
    "df.drop('Race', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Sets upper boundary of 90 hours work week in NumWorkingHoursPerWeek and fills with mean value\n",
    "modeWorkHours = df['NumWorkingHoursPerWeek'].mode()[0]\n",
    "df['NumWorkingHoursPerWeek'] = df['NumWorkingHoursPerWeek'].where(df['NumWorkingHoursPerWeek'] <= 90, modeWorkHours)  \n",
    "# Round off hours                             \n",
    "df['NumWorkingHoursPerWeek'] = df['NumWorkingHoursPerWeek'].astype(int)\n",
    "\n",
    "# Replaces missing data with mean of column\n",
    "df['Weighting'] = df['Weighting'].fillna(df['Weighting'].mean())\n",
    "\n",
    "\n",
    "# Replaces missing data with unknown Occupation\n",
    "df['Occupation'] = df['Occupation'].fillna(\"?\")\n",
    "\n",
    "# Replaces unknowns with the mode of attribute\n",
    "# By WorkClass\n",
    "replaceWithMode(df,'Occupation','WorkClass','Federal-gov')\n",
    "replaceWithMode(df,'Occupation','WorkClass','Self-emp-inc')\n",
    "replaceWithMode(df,'Occupation','WorkClass','Private')\n",
    "replaceWithMode(df,'Occupation','WorkClass','Never-worked')\n",
    "\n",
    "# By education\n",
    "for values in df['Education'].unique():\n",
    "    replaceWithMode(df, 'Occupation', 'Education', values)\n",
    "    replaceWithMode(df, 'WorkClass', 'Education', values)\n",
    "\n",
    "\n",
    "# Never-worked adults will have occupation set as other services as a generic unknown\n",
    "df['Occupation'] = df['Occupation'].mask(((df['WorkClass']=='Never-worked') & (df['Occupation']=='?')), \"Other-service\")\n",
    "\n",
    "\n",
    "# Turning CapitalAvg into binary options \n",
    "df['CapitalAvg'][df['CapitalAvg'] > 0] = 1\n",
    "df['CapitalAvg'] = df['CapitalAvg'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "5        1\n",
      "6        1\n",
      "7        1\n",
      "8        1\n",
      "9        1\n",
      "10       1\n",
      "11       1\n",
      "12       1\n",
      "13       1\n",
      "14       1\n",
      "15       1\n",
      "16       2\n",
      "17       1\n",
      "18       1\n",
      "19       1\n",
      "20       1\n",
      "21       1\n",
      "22       1\n",
      "23       1\n",
      "24       1\n",
      "25       2\n",
      "26       1\n",
      "27       1\n",
      "28       1\n",
      "29       1\n",
      "        ..\n",
      "39969    1\n",
      "39970    1\n",
      "39971    1\n",
      "39972    1\n",
      "39973    1\n",
      "39974    1\n",
      "39975    1\n",
      "39976    1\n",
      "39977    1\n",
      "39978    1\n",
      "39979    1\n",
      "39980    1\n",
      "39981    1\n",
      "39982    2\n",
      "39983    1\n",
      "39984    1\n",
      "39985    1\n",
      "39986    1\n",
      "39987    1\n",
      "39988    1\n",
      "39989    1\n",
      "39990    1\n",
      "39991    1\n",
      "39992    1\n",
      "39993    1\n",
      "39994    1\n",
      "39995    1\n",
      "39996    1\n",
      "39997    1\n",
      "39998    1\n",
      "Name: Weighting, Length: 39027, dtype: category\n",
      "Categories (4, int64): [1 < 2 < 3 < 4]\n"
     ]
    }
   ],
   "source": [
    "# Converting weight to 0 and 1 based on median\n",
    "weightMax = df['Weighting'].max()\n",
    "#df['Weighting'][df['Weighting'] < weightMedian] = 0\n",
    "#df['Weighting'][df['Weighting'] > weightMedian] = 1\n",
    "#df['Weighting'] = df['Weighting'].astype(int)\n",
    "bins = [0, weightMax/4, weightMax/2, weightMax*3/4,  weightMax]\n",
    "labels =[1,2,3,4]\n",
    "df['Weighting'] = pd.cut(df['Weighting'], bins,labels=labels)\n",
    "print (df['Weighting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10425.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Weighting'].min()\n",
    "#df['Weighting'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Mappings\n",
    "\n",
    "atriskclassMapping = {'High': 1, 'Low':0} #Binary\n",
    "\n",
    "workclassMapping = {'Private':0, 'Local-gov': 1, 'Self-emp-not-inc':2, 'Federal-gov': 3, 'State-gov': 4,  'Self-emp-inc': 5\\\n",
    "                    ,'Without-pay':6, 'Never-worked':7}\n",
    "\n",
    "educationclassMapping = {'Preschool':0,'1st-4th': 1, '5th-6th': 2, '7th-8th': 3, '9th': 4, '10th': 5,\n",
    "       '11th':6, '12th':7,'HS-grad':8, 'Some-college':9, 'Bachelors':10, 'Prof-school':11, 'Masters': 12, 'Doctorate': 13,\n",
    "                         'Assoc-acdm': 14, 'Assoc-voc': 15} #Ordinal?? not sure of order\n",
    "\n",
    "maritalstatusclassMapping = {'Never-married': 0, 'Married-civ-spouse': 1, 'Widowed': 2, 'Divorced':3,\n",
    "       'Separated':4, 'Married-spouse-absent':5, 'Married-AF-spouse':6 }\n",
    "\n",
    "occupationclassMapping = {'Machine-op-inspct': 0 , 'Farming-fishing': 1, 'Protective-serv': 2,\n",
    "       'Adm-clerical': 3, 'Other-service': 4, 'Craft-repair': 5, 'Prof-specialty': 6,\n",
    "       'Exec-managerial': 7, 'Tech-support': 8, 'Sales': 9, 'Priv-house-serv': 10,\n",
    "       'Transport-moving': 11, 'Handlers-cleaners': 12, 'Armed-Forces': 13}\n",
    "\n",
    "relationshipclassMapping = {'Own-child': 0, 'Husband':1, 'Not-in-family':2, 'Unmarried':3, 'Wife':4,\n",
    "       'Other-relative':5}\n",
    "\n",
    "countryclassMapping = {'USA':0, 'Peru':1, 'Guatemala':2, 'Mexico':3, 'Dominican-Republic':4,\n",
    "       'Ireland':5, 'Germany':6, 'Philippines':7, 'Thailand':8, 'Haiti':9,\n",
    "       'El-Salvador':10, 'Puerto-Rico':11, 'Vietnam':12, 'South':13, 'Columbia':14,\n",
    "       'Japan':15, 'India':16, 'Cambodia':17, 'Poland':18, 'Laos':19, 'England':20, 'Cuba':21,\n",
    "       'Taiwan':22, 'Italy':23, 'Canada':24, 'Portugal':25, 'China':26, 'Nicaragua':27,\n",
    "       'Honduras':28, 'Iran':29, 'Scotland':30, 'Jamaica':31, 'Ecuador':32, 'Yugoslavia':33,\n",
    "       'Hungary':34, 'Hong':35, 'Greece':36, 'Trinadad&Tobago':37,\n",
    "       'Outlying-US(Guam-USVI-etc)':38, 'France':39, 'Holand-Netherlands':40}\n",
    "\n",
    "df['AtRisk'] = df['AtRisk'].map(atriskclassMapping).astype(int)\n",
    "df['WorkClass'] = df['WorkClass'].map(workclassMapping).astype(int)\n",
    "df['Education'] = df['Education'].map(educationclassMapping).astype(int)\n",
    "df['Marital-Status'] = df['Marital-Status'].map(maritalstatusclassMapping).astype(int)\n",
    "df['Occupation'] = df['Occupation'].map(occupationclassMapping).astype(int)\n",
    "df['Relationship'] = df['Relationship'].map(relationshipclassMapping).astype(int)\n",
    "df['CountryOfOrigin'] = df['CountryOfOrigin'].map(countryclassMapping).astype(int)\n",
    "df['NumYearsEducation']= df['NumYearsEducation'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Predictive Modelling using Decision Trees\n",
    "\n",
    "**Dataset is split into target and input types**\n",
    "\n",
    "By dropping ID, target variable and other object types, a decision tree can be formed.\n",
    "\n",
    "Cross-fold validation with k=10 is done to the training set of 70%, having 7% for each fold. Then, it is fitted and ready for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=10,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target/input split\n",
    "y = df['AtRisk']\n",
    "# Drop all object type (temporarily)\n",
    "X = df.drop(['ID', 'CapitalLoss', 'CapitalGain', 'AtRisk'], axis=1)\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=10)\n",
    "\n",
    "# simple decision tree training\n",
    "clf = DecisionTreeClassifier(random_state=10)\n",
    "K_fold = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 \n",
    "\n",
    "**1a) What is classification accuracy on training and test datasets?**\n",
    "\n",
    "**b) Which variable is used for the first split? What are the variables that are used for the second split?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9752910169119262\n",
      "Test accuracy: 0.787086856264412\n",
      "Age : 0.22401632445482167\n",
      "Marital-Status : 0.21261891250800627\n",
      "NumYearsEducation : 0.15220950991217747\n",
      "NumWorkingHoursPerWeek : 0.12059083654744511\n",
      "Occupation : 0.09587026148825693\n",
      "WorkClass : 0.05449480734517138\n",
      "CapitalAvg : 0.04697049002341591\n",
      "CountryOfOrigin : 0.02442647959490605\n",
      "Education : 0.023552104952990923\n",
      "Relationship : 0.019764736099724945\n",
      "Sex : 0.012874378730231105\n",
      "Weighting : 0.012611158342852319\n"
     ]
    }
   ],
   "source": [
    "# Task 2 1a\n",
    "print(\"Train accuracy:\", clf.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", clf.score(X_test, y_test))\n",
    "\n",
    "# Task 2 1b\n",
    "\n",
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = clf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "**2) Build another decision tree tuned with GridSearchCV**\n",
    "\n",
    "With this, we have set the hyperparameters as the maximum depth of tree, and test against depths of 1 to 100, with cross fold validation of k=5. Then, the accuracy and scores are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=5)\n",
    "tree_depth = np.arange(10,100)\n",
    "sample_split = np.arange(2,5)\n",
    "\n",
    "gs = GridSearchCV(clf, param_grid={'max_depth':tree_depth, 'min_samples_split':sample_split}, iid=True, cv=5)\n",
    "\n",
    "K_fold_prediction = cross_val_score(gs, X_train, y_train, cv=3)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (K_fold_prediction.mean(), K_fold_prediction.std() * 2))\n",
    "print(\"Cross validation scores are:\", K_fold_prediction)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"The training set gave a best score of \" + str(gs.best_score_))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_scores = validation_curve(\n",
    "    DecisionTreeClassifier(random_state=5), X, y, param_name=\"max_depth\", param_range=tree_depth,\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with DT\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(tree_depth, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(tree_depth, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(tree_depth, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(tree_depth, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 2a\n",
    "print(\"Train accuracy:\", gs.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", gs.score(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
