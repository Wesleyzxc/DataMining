{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all packages and define functions used for data-preprocessing \n",
    "# Reads the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import all packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import tree, neighbors, svm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "import warnings\n",
    "# To ignore any future warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Repeated functions\n",
    "def getMode(df, attribute, condAtt, cond):\n",
    "    return (df[attribute][df[condAtt]==cond].mode()[0])\n",
    "\n",
    "def replaceWithMode(df, attribute, condAtt, cond):\n",
    "    mode = getMode(df, attribute, condAtt, cond)\n",
    "    df[attribute] = df[attribute].mask(((df[condAtt]==cond) & (df[attribute]=='?')), mode)\n",
    "    \n",
    "\n",
    "# read the dataset and set skipinitialspace to true to be able to .replace\n",
    "df = pd.read_csv('./HouseholderAtRisk(1).csv', skipinitialspace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 \n",
    "**1)      What proportion of households who have high risk?**\n",
    "\n",
    "Properties with high risks are calculated with value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of high risk = 0.7624690617265432\n"
     ]
    }
   ],
   "source": [
    "# Task 1 question 1\n",
    "# Show proportion of high risk\n",
    "risks = df['AtRisk'].value_counts()\n",
    "print(\"Proportion of high risk = \" + str(risks[0]/len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "**2) Did you have to fix any data quality problems? Detail them?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with multiple columns containing NaN values\n",
    "df = df.dropna(subset=[\"Relationship\", \"Sex\", \"NumYearsEducation\"], how='all')\n",
    "\n",
    "# Replace inconsistency in CountryOfOrigin\n",
    "df['CountryOfOrigin'] = df['CountryOfOrigin'].replace(\"US\", \"USA\").replace(\"United-States\", \"USA\")\n",
    "\n",
    "##Replace ? to mode in CountryOfOrigin\n",
    "modeCountry = df['CountryOfOrigin'].mode()[0]\n",
    "df.loc[df['CountryOfOrigin']=='?', 'CountryOfOrigin'] = modeCountry\n",
    "\n",
    "# Removing Gender to use numerical binary for Sex where 0 = Male, 1 = Female\n",
    "df.drop('Gender', axis=1, inplace=True)\n",
    "\n",
    "# Replace -1 value in Age with mean value\n",
    "modeAge = df['Age'].mode()[0]\n",
    "df.loc[df['Age']==-1, 'Age'] = modeAge\n",
    "\n",
    "# Round off age\n",
    "df['Age'] = df['Age'].astype(int)\n",
    "\n",
    "# Drop race as there are 39954 NaN vs 45 labelled classes\n",
    "df.drop('Race', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Sets upper boundary of 90 hours work week in NumWorkingHoursPerWeek and fills with mean value\n",
    "modeWorkHours = df['NumWorkingHoursPerWeek'].mode()[0]\n",
    "df['NumWorkingHoursPerWeek'] = df['NumWorkingHoursPerWeek'].where(df['NumWorkingHoursPerWeek'] <= 90, modeWorkHours)  \n",
    "# Round off hours                             \n",
    "df['NumWorkingHoursPerWeek'] = df['NumWorkingHoursPerWeek'].astype(int)\n",
    "\n",
    "# Replaces missing data with mean of column\n",
    "df['Weighting'] = df['Weighting'].fillna(df['Weighting'].mean())\n",
    "\n",
    "\n",
    "# Replaces missing data with unknown Occupation\n",
    "df['Occupation'] = df['Occupation'].fillna(\"?\")\n",
    "\n",
    "# Replaces unknowns with the mode of attribute\n",
    "# By WorkClass\n",
    "replaceWithMode(df,'Occupation','WorkClass','Federal-gov')\n",
    "replaceWithMode(df,'Occupation','WorkClass','Self-emp-inc')\n",
    "replaceWithMode(df,'Occupation','WorkClass','Private')\n",
    "replaceWithMode(df,'Occupation','WorkClass','Never-worked')\n",
    "\n",
    "# By education\n",
    "for values in df['Education'].unique():\n",
    "    replaceWithMode(df, 'Occupation', 'Education', values)\n",
    "    replaceWithMode(df, 'WorkClass', 'Education', values)\n",
    "\n",
    "\n",
    "# Never-worked adults will have occupation set as other services as a generic unknown\n",
    "df['Occupation'] = df['Occupation'].mask(((df['WorkClass']=='Never-worked') & (df['Occupation']=='?')), \"Other-service\")\n",
    "\n",
    "###Mappings\n",
    "\n",
    "atriskclassMapping = {'High': 1, 'Low':0} #Binary\n",
    "\n",
    "workclassMapping = {'Private':0, 'Local-gov': 1, 'Self-emp-not-inc':2, 'Federal-gov': 3, 'State-gov': 4,  'Self-emp-inc': 5\\\n",
    "                    ,'Without-pay':6, 'Never-worked':7}\n",
    "\n",
    "educationclassMapping = {'Preschool':0,'1st-4th': 1, '5th-6th': 2, '7th-8th': 3, '9th': 4, '10th': 5,\n",
    "       '11th':6, '12th':7,'HS-grad':8, 'Some-college':9, 'Bachelors':10, 'Prof-school':11, 'Masters': 12, 'Doctorate': 13,\n",
    "                         'Assoc-acdm': 14, 'Assoc-voc': 15} #Ordinal?? not sure of order\n",
    "\n",
    "maritalstatusclassMapping = {'Never-married': 0, 'Married-civ-spouse': 1, 'Widowed': 2, 'Divorced':3,\n",
    "       'Separated':4, 'Married-spouse-absent':5, 'Married-AF-spouse':6 }\n",
    "\n",
    "occupationclassMapping = {'Machine-op-inspct': 0 , 'Farming-fishing': 1, 'Protective-serv': 2,\n",
    "       'Adm-clerical': 3, 'Other-service': 4, 'Craft-repair': 5, 'Prof-specialty': 6,\n",
    "       'Exec-managerial': 7, 'Tech-support': 8, 'Sales': 9, 'Priv-house-serv': 10,\n",
    "       'Transport-moving': 11, 'Handlers-cleaners': 12, 'Armed-Forces': 13}\n",
    "\n",
    "relationshipclassMapping = {'Own-child': 0, 'Husband':1, 'Not-in-family':2, 'Unmarried':3, 'Wife':4,\n",
    "       'Other-relative':5}\n",
    "\n",
    "countryclassMapping = {'USA':1, 'Peru':1, 'Guatemala':1, 'Mexico':1, 'Dominican-Republic':1,\n",
    "       'Ireland':2, 'Germany':2, 'Philippines':3, 'Thailand':3, 'Haiti':1,\n",
    "       'El-Salvador':1, 'Puerto-Rico':1, 'Vietnam':3, 'South':3, 'Columbia':1,\n",
    "       'Japan':3, 'India':4, 'Cambodia':1, 'Poland':2, 'Laos':3, 'England':2, 'Cuba':1,\n",
    "       'Taiwan':2, 'Italy':2, 'Canada':1, 'Portugal':2, 'China':3, 'Nicaragua':1,\n",
    "       'Honduras':1, 'Iran':4, 'Scotland':2, 'Jamaica':1, 'Ecuador':1, 'Yugoslavia':1,\n",
    "       'Hungary':2, 'Hong':3, 'Greece':2, 'Trinadad&Tobago':1,\n",
    "       'Outlying-US(Guam-USVI-etc)':1, 'France':2, 'Holand-Netherlands':2}\n",
    "\n",
    "df['AtRisk'] = df['AtRisk'].map(atriskclassMapping).astype(int)\n",
    "df['WorkClass'] = df['WorkClass'].map(workclassMapping).astype(int)\n",
    "df['Education'] = df['Education'].map(educationclassMapping).astype(int)\n",
    "df['Marital-Status'] = df['Marital-Status'].map(maritalstatusclassMapping).astype(int)\n",
    "df['Occupation'] = df['Occupation'].map(occupationclassMapping).astype(int)\n",
    "df['Relationship'] = df['Relationship'].map(relationshipclassMapping).astype(int)\n",
    "df['CountryOfOrigin'] = df['CountryOfOrigin'].map(countryclassMapping).astype(int)\n",
    "df['NumYearsEducation']= df['NumYearsEducation'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning CapitalAvg into binary options \n",
    "df['CapitalAvg'][df['CapitalAvg'] > 0] = 1\n",
    "df['CapitalAvg'] = df['CapitalAvg'].astype(int)\n",
    "\n",
    "# Converting weight from 1 to 4 based on equal intervals (equi-width)\n",
    "weightMax = df['Weighting'].max()\n",
    "bins = [0, weightMax/4, weightMax/2, weightMax*3/4,  weightMax+1]\n",
    "labels =[1,2,3,4]\n",
    "df['Weighting'] = pd.cut(df['Weighting'], bins,labels=labels)\n",
    "print (df['Weighting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting age from 1 to 4 based on equal intervals (equi-width)\n",
    "df['Age'].max()\n",
    "ageMax = df['Age'].max()\n",
    "bins = [0, ageMax/4, ageMax/2, ageMax*3/4,  ageMax+1]\n",
    "labels =[1,2,3,4]\n",
    "df['Age'] = pd.cut(df['Age'], bins,labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting education years from 1 to 4 based on equal intervals (equi-width)\n",
    "df['NumYearsEducation'].max()\n",
    "eduMax = df['NumYearsEducation'].max()\n",
    "bins = [0, eduMax/4, eduMax/2, eduMax*3/4,  eduMax+1]\n",
    "labels =[1,2,3,4]\n",
    "df['NumYearsEducation'] = pd.cut(df['NumYearsEducation'], bins,labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Predictive Modelling using Decision Trees\n",
    "\n",
    "**Dataset is split into target and input types**\n",
    "\n",
    "By dropping ID, target variable and other object types, a decision tree can be formed.\n",
    "\n",
    "Cross-fold validation with k=10 is done to the training set of 70%, having 7% for each fold. Then, it is fitted and ready for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=10,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target/input split\n",
    "y = df['AtRisk']\n",
    "# Drop all object type\n",
    "X = df.drop(['ID', 'CapitalLoss', 'CapitalGain', 'AtRisk'], axis=1)\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=10)\n",
    "\n",
    "# simple decision tree training\n",
    "clf = DecisionTreeClassifier(random_state=10)\n",
    "K_fold = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 \n",
    "\n",
    "**1a) What is classification accuracy on training and test datasets?**\n",
    "\n",
    "**b) Which variable is used for the first split? What are the variables that are used for the second split?**\n",
    "\n",
    "Before binning age: 0.76~78\n",
    "\n",
    "After binning age: 0.798\n",
    "\n",
    "After binning age and weighting: 0.7975\n",
    "\n",
    "And numyearsedu: 0.79776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 1a\n",
    "print(\"Train accuracy:\", clf.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", clf.score(X_test, y_test))\n",
    "\n",
    "# Task 2 1b\n",
    "\n",
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = clf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "**2) Build another decision tree tuned with GridSearchCV**\n",
    "\n",
    "With this, we have set the hyperparameters as the maximum depth of tree, and test against depths of 1 to 100, with cross fold validation of k=5. Then, the accuracy and scores are printed.\n",
    "\n",
    "Before binning age: 0.8360<br>\n",
    "After binning age: 0.83659<br>\n",
    "After binning age and weighting: 0.83673<br>\n",
    "And numyearsedu: 0.83498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 20),\n",
    "          'min_samples_leaf': range(200, 600, 100)}\n",
    "tree_depth = np.arange(1,100)\n",
    "\n",
    "gs = GridSearchCV(clf, param_grid=params, iid=True, cv=5)\n",
    "\n",
    "#K_fold_prediction = cross_val_score(gs, X_train, y_train, cv=3)\n",
    "#print(\"Accuracy: %0.2f (+/- %0.2f)\" % (K_fold_prediction.mean(), K_fold_prediction.std() * 2))\n",
    "#print(\"Cross validation scores are:\", K_fold_prediction)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"The training set gave a best score of \" + str(gs.best_score_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameter\n",
    "\n",
    "print (str(gs.best_params_))\n",
    "print (str(gs.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Variables\n",
    "#get attribute scores from best model\n",
    "best_est = gs.best_estimator_\n",
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = best_est.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree_depth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d85909f592ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m train_scores, test_scores = validation_curve(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"max_depth\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtree_depth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     cv=5, scoring=\"accuracy\", n_jobs=1)\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_scores_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tree_depth' is not defined"
     ]
    }
   ],
   "source": [
    "train_scores, test_scores = validation_curve(\n",
    "    DecisionTreeClassifier(random_state=5), X, y, param_name=\"max_depth\", param_range=tree_depth,\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with DT\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(tree_depth, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(tree_depth, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(tree_depth, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(tree_depth, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 2a\n",
    "print(\"Train accuracy:\", gs.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", gs.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8219122922615125\n",
      "Test accuracy: 0.8236399350926638\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# target/input split\n",
    "y = df['AtRisk']\n",
    "# Drop all object type\n",
    "X = df.drop(['ID', 'CapitalLoss', 'CapitalGain', 'AtRisk'], axis=1)\n",
    "\n",
    "X_mat = X.as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.3, stratify=y, random_state=10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test) # don't fit this\n",
    "\n",
    "model = LogisticRegression(random_state=10)\n",
    "\n",
    "# fit it to training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# training and test accuracy\n",
    "print(\"Train accuracy:\", model.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(X_test, y_test))\n",
    "\n",
    "#age weighting numyearseducation\n",
    "#capitalavg numworkinghrspreweek \n",
    "# are continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns from X\n",
    "cols = X.columns\n",
    "cols = [col for col in cols]\n",
    "cols.remove('Sex')\n",
    "cols.remove('CountryOfOrigin')\n",
    "cols.remove('Marital-Status')\n",
    "cols.remove('Occupation')\n",
    "cols.remove('Relationship')\n",
    "cols.remove('WorkClass')\n",
    "cols.remove('Education')\n",
    "cols.remove('Weighting')\n",
    "print(cols)\n",
    "\n",
    "# visualise the columns\n",
    "fig, ax = plt.subplots(3,4, figsize=(10,10), sharex=False)\n",
    "\n",
    "# draw distplots on each inspected column in X\n",
    "for i, col in enumerate(cols):\n",
    "    sns.distplot(X[col].dropna(), hist=False, ax=ax[int(i/4)][i%4])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
